{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Analytics - Programming Task 1 {-}\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment Points**: 100  \n",
    "**Due Date**: Friday Week 6 (4 April 2025) @ 11.59pm  \n",
    "**Submission**: Provide your answers in this Jupypter notebook and submit it via iLearn link  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predictive Analysis of Credit Card Defaults**\n",
    "\n",
    "### Objective: \n",
    "- This assignment focuses on a dataset of customers' default payments.\n",
    "- The primary goal is to predict which credit card clients are likely to default using various data mining methods.\n",
    "\n",
    "### Background: \n",
    "Traditional risk management models classify clients as either credible or not credible based on their likelihood of default. This project aims to refine this classification by identifying specific individuals who are likely to default, enhancing the precision of credit risk assessments.\n",
    "\n",
    "Target variable\n",
    "- default.payment.next.month: Default payment (1=yes, 0=no)\n",
    "\n",
    "The dataset contains the following features\n",
    "\n",
    "1. ID: ID of each client\n",
    "2. LIMIT_BAL: Amount of given credit in dollars (includes individual and family/supplementary credit\n",
    "3. SEX: Gender (1=male, 2=female)\n",
    "4. EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
    "5. MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
    "6. AGE: Age in years\n",
    "7. PAY_0: Repayment status in September (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "8. PAY_2: Repayment status in August (scale same as above)\n",
    "9. PAY_3: Repayment status in July, (scale same as above)\n",
    "10. PAY_4: Repayment status in June (scale same as above)\n",
    "11. PAY_5: Repayment status in May (scale same as above)\n",
    "12. PAY_6: Repayment status in April (scale same as above)\n",
    "13. BILL_AMT1: Amount of bill statement in September (dollars)  \n",
    "14. BILL_AMT2: Amount of bill statement in August (dollars)  \n",
    "15. BILL_AMT3: Amount of bill statement in July (dollars)  \n",
    "16. BILL_AMT4: Amount of bill statement in June (dollars)  \n",
    "17. BILL_AMT5: Amount of bill statement in May (dollars)  \n",
    "18. BILL_AMT6: Amount of bill statement in April (dollars)   \n",
    "19. PAY_AMT1: Amount of previous payment in September (dollars)  \n",
    "20. PAY_AMT2: Amount of previous payment in August (dollars)  \n",
    "21. PAY_AMT3: Amount of previous payment in July (dollars)   \n",
    "22. PAY_AMT4: Amount of previous payment in June (dollars)  \n",
    "23. PAY_AMT5: Amount of previous payment in May (dollars)   \n",
    "24. PAY_AMT6: Amount of previous payment in April (dollars)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1** - Reading the dataset (Total Marks: 20)\n",
    "\n",
    "\n",
    "\n",
    "**Q1**. Create a pandas dataframe contining the first 10,000 rows from the credit card dataset provided in the **assignment_data** folder \n",
    "- Delete 'ID' column\n",
    "- Print `info()` of the dataframe \n",
    "\n",
    "(5 marks) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   LIMIT_BAL                   10000 non-null  int64  \n",
      " 1   SEX                         9617 non-null   float64\n",
      " 2   EDUCATION                   9617 non-null   float64\n",
      " 3   MARRIAGE                    9617 non-null   float64\n",
      " 4   AGE                         9617 non-null   float64\n",
      " 5   PAY_0                       9617 non-null   float64\n",
      " 6   PAY_2                       9617 non-null   float64\n",
      " 7   PAY_3                       9617 non-null   float64\n",
      " 8   PAY_4                       9617 non-null   float64\n",
      " 9   PAY_5                       9617 non-null   float64\n",
      " 10  PAY_6                       9642 non-null   float64\n",
      " 11  BILL_AMT1                   9642 non-null   float64\n",
      " 12  BILL_AMT2                   9642 non-null   float64\n",
      " 13  BILL_AMT3                   9642 non-null   float64\n",
      " 14  BILL_AMT4                   9642 non-null   float64\n",
      " 15  BILL_AMT5                   9632 non-null   float64\n",
      " 16  BILL_AMT6                   9632 non-null   float64\n",
      " 17  PAY_AMT1                    9632 non-null   float64\n",
      " 18  PAY_AMT2                    9632 non-null   float64\n",
      " 19  PAY_AMT3                    9990 non-null   float64\n",
      " 20  PAY_AMT4                    9990 non-null   float64\n",
      " 21  PAY_AMT5                    9710 non-null   float64\n",
      " 22  PAY_AMT6                    9710 non-null   float64\n",
      " 23  default payment next month  10000 non-null  int64  \n",
      "dtypes: float64(22), int64(2)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "import pandas as pd\n",
    "# 10,000 rows of the dataset\n",
    "df = pd.read_excel(\"assignment_data/credit_data.xlsx\", nrows=10000)\n",
    "\n",
    "# Drop ID \n",
    "df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Display the structure\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. List which **features** are *numeric*, *ordinal*, and *nominal* variables, and how many features of each kind there are in the dataset.\n",
    "To answer this question \n",
    "\n",
    "- Find the definitions of numeric, ordinal and nominal variables in the course material    \n",
    "- Carefully consider what values each feature can take as well as the output of `df.info()`. \n",
    "\n",
    "Your answer should be written up in Markdown and include:\n",
    "1) A table listing all the features present in the dataset and their type (fill out the table template provided below) and\n",
    "2) A brief description of the contents of the table.\n",
    "\n",
    "|Variable Kind|Number of Features|Feature Names\n",
    "| --- | --- | --- |\n",
    "| Numeric | some number | some text |\n",
    "| some text  | some number | some text |\n",
    "| some text  | some number | some text |\n",
    "\n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The dataset consists of 24 features (after dropping the 'ID' column), which can be categorized based on their data type and meaning into numeric, ordinal, or nominal variables.\n",
    "\n",
    "| Variable Kind | Number of Features | Feature Names |\n",
    "|---------------|--------------------|----------------|\n",
    "| Numeric       | 14                 | LIMIT_BAL, AGE, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6 |\n",
    "| Ordinal       | 7                  | EDUCATION, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6 |\n",
    "| Nominal       | 3                  | SEX, MARRIAGE, default payment next month |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "Variables were grouped based on how they are used and what they represent:\n",
    "\n",
    "- **Numeric**: These are actual numbers like credit limit, age, bill amounts, and past payments. They can be measured and used in calculations.\n",
    "\n",
    "- **Ordinal**: These are categories with order. For example, education level and repayment status have a clear ranking, even if the gaps between values aren't equal.\n",
    "\n",
    "- **Nominal**: These are labels with no order, like gender and default payment, marriage is also part of nominal because there is no order in marital status.\n",
    "\n",
    "This classification helps us understand how to handle each type during analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3.** Missing Values. \n",
    "\n",
    "- Print out the number of missing values for each variable in the dataset and comment on your findings.\n",
    "\n",
    "(5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIMIT_BAL                       0\n",
      "SEX                           383\n",
      "EDUCATION                     383\n",
      "MARRIAGE                      383\n",
      "AGE                           383\n",
      "PAY_0                         383\n",
      "PAY_2                         383\n",
      "PAY_3                         383\n",
      "PAY_4                         383\n",
      "PAY_5                         383\n",
      "PAY_6                         358\n",
      "BILL_AMT1                     358\n",
      "BILL_AMT2                     358\n",
      "BILL_AMT3                     358\n",
      "BILL_AMT4                     358\n",
      "BILL_AMT5                     368\n",
      "BILL_AMT6                     368\n",
      "PAY_AMT1                      368\n",
      "PAY_AMT2                      368\n",
      "PAY_AMT3                       10\n",
      "PAY_AMT4                       10\n",
      "PAY_AMT5                      290\n",
      "PAY_AMT6                      290\n",
      "default payment next month      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "# Print the number of missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "The result shows that some variables like SEX, EDUCATION, MARRIAGE, AGE, and PAY_0 to PAY_5 have 383 missing values. Other features like BILL_AMT and PAY_AMT also have hundreds of missing values, while the target variable has no missing.\r\n",
    "\r\n",
    "This condition shows tha**handling missing value* is needed before doing modeling. But in some cases, dropping rows or columns can also be an option, depending on how many missing and how important the feature is.\r\n",
    "\r\n",
    "In conclusion, imputation is the main strategy here because many important features have a significant amount of missing dal.\r\n",
    "\r\n",
    "We will handle missing values using:\r\n",
    "- **Mean** for numeric features\r\n",
    "- **Mode** for categorical and ordinal features\r\n",
    "\r\n",
    "This ensures the dataset remains complete without dropping any rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 2.** Cleaning data and dealing with categorical features (Total Marks: 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** \n",
    "\n",
    "- Use an appropriate `pandas` function to impute missing values using one of the following two strategies: `mean` and `mode`. (10 marks)\n",
    "    - Take into consideration the type of each variable (as in Q2 above) and the best practices we discussed in class/lecture notes\n",
    "- Explain what data imputation is, how you have done it here, and what decisions you had to make. (5 marks)\n",
    "\n",
    "\n",
    "(Total: 15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "# Use mean for numeric variables\n",
    "numeric_cols = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3',\n",
    "                'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "                'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3',\n",
    "                'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6','AGE']\n",
    "\n",
    "# Use mode for categorical and ordinal variables\n",
    "categorical_cols = ['SEX', 'EDUCATION', 'MARRIAGE',\n",
    "                    'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "\n",
    "# Impute numeric columns with mean\n",
    "for col in numeric_cols:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "# Impute categorical/ordinal columns with mode\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "Missing value imputation is an important step to ensure the dataset is complete and ready for analysis. In this task, I used two imputation methods: mean and mode, selected based on the type of each variable.\n",
    "\n",
    "- **Mean** was used for numerical variables such as LIMIT_BAL, AGE, BILL_AMT, and PAY_AMT.  \n",
    "  This method is suitable for continuous data and helps maintain the overall distribution.\n",
    "\n",
    "- **Mode** was applied to categorical and ordinal variables like SEX, EDUCATION, MARRIAGE, and PAY_.  \n",
    "  Using the most frequent value ensures that categorical integrity is preserved.\n",
    "\n",
    "These decisions were made by considering:\n",
    "- The nature of each variable\n",
    "- Best practices discussed during lectures\n",
    "\n",
    "With these imputations, the dataset is now ready for modeling without losing important patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. \n",
    "- Print `value_counts()` of the 'SEX' column and add a dummy variable named 'SEX_FEMALE' to `df` using `get_dummies()` (3 marks)\n",
    "- Carefully explain what the values of the new variable 'SEX_FEMALE' mean (2 mark)\n",
    "- Make sure the variable 'SEX' is deleted from the original dataframe   \n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEX\n",
      "2.0    6162\n",
      "1.0    3838\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "# Print value counts of the SEX column\n",
    "sex_counts = df['SEX'].value_counts()\n",
    "print(df['SEX'].value_counts())\n",
    "\n",
    "# Create dummy variable for SEX and rename column\n",
    "df = pd.get_dummies(df, columns=['SEX'], drop_first=True)\n",
    "\n",
    "df.rename(columns={'SEX_2.0': 'SEX_FEMALE'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "Based on the value_counts() output, the SEX column contains two values:\n",
    "- **2.0** with **6,162** records\n",
    "- **1.0** with **3,838** records\n",
    "\n",
    "To prepare this categorical variable for modeling, I created a dummy variable using get_dummies() with drop_first=True. This created one dummy column, originally named SEX_2.0, which I renamed to SEX_FEMALE for clarity.\n",
    "\n",
    "**Meaning of `SEX_FEMALE`:**\n",
    "- A value of **1** indicates the person is **female (SEX = 2)**\n",
    "- A value of **0** indicates the person is **male (SEX = 1)**\n",
    "\n",
    "After creating the dummy variable, the original SEX column was automatically removed from the dataframe.  \n",
    "This transformation ensures the data is in a numerical format suitable for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3**. Print `value_counts()` of the 'MARRIAGE' column and *carefully* comment on what you notice in relation to the definition of this variable. \n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MARRIAGE\n",
       "2.0    5518\n",
       "1.0    4380\n",
       "3.0      82\n",
       "0.0      20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "df['MARRIAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "The value_counts() output for the **MARRIAGE** column shows the following distribution:\n",
    "\n",
    "- **1.0** → 5,518 records (**married**)  \n",
    "- **2.0** → 4,380 records (**single**)  \n",
    "- **3.0** → 82 records (**others**)  \n",
    "- **0.0** → 20 records (**invalid or undefined**)\n",
    "\n",
    "According to the dataset documentation, the valid values for **MARRIAGE** are:\n",
    "- **1 = Married**\n",
    "- **2 = Single**\n",
    "- **3 = Others**\n",
    "\n",
    "The presence of **0.0** indicates a data entry error or an undefined category that does not match the official label definitions. Although it appears in only a small number of cases, this value should be addressed to avoid issues during preprocessing.\n",
    "\n",
    "One reasonable approach would be to group **0.0** under the **\"Others\" (3.0)** category, in order to maintain consistency in the categorical structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q4**. \n",
    "\n",
    "- Apply `get_dummies()` to 'MARRIAGE' feature and add dummy variables 'MARRIAGE_MARRIED', 'MARRIAGE_SINGLE', 'MARRIAGE_OTHER' to `df`. (5 marks)   \n",
    "- *Carefully consider* how to allocate all the values of 'MARRIAGE' across these 3 newly created features (5 marks)\n",
    "    - Do not delete observations \n",
    "    - Do not assume that the anomaly are missing observations      \n",
    "    - Explain what decision you made\n",
    "- Make sure that 'MARRIAGE' is deleted from `df`   \n",
    "\n",
    "(Total: 10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "# Group undefined value 0 into category 3 (Others)\n",
    "df['MARRIAGE'] = df['MARRIAGE'].replace(0, 3)\n",
    "\n",
    "# Create dummy variables for MARRIAGE\n",
    "df = pd.get_dummies(df, columns=['MARRIAGE'], prefix='MARRIAGE')\n",
    "\n",
    "# Rename dummy variable y\n",
    "df.rename(columns={\n",
    "    'MARRIAGE_1.0': 'MARRIAGE_MARRIED',\n",
    "    'MARRIAGE_2.0': 'MARRIAGE_SINGLE',\n",
    "    'MARRIAGE_3.0': 'MARRIAGE_OTHER'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "Applied get_dummies() to the MARRIAGE feature to convert it into three dummy variables: **MARRIAGE_MARRIED**, **MARRIAGE_SINGLE**, and **MARRIAGE_OTHER**.\n",
    "\n",
    "Before creating the dummies, I identified that the **MARRIAGE** column contained a value **0.0**, which is not part of the official categories (1 = Married, 2 = Single, 3 = Others).  \n",
    "As instructed, I did not treat this value as missing, nor delete any observations. I decided to **reassign the value 0.0 into category 3 (Others)**, as it is undefined and appears in a very small portion of the data.\n",
    "\n",
    "After using get_dummies(), I renamed the dummy columns for clarity:\n",
    "- MARRIAGE_1.0 → **MARRIAGE_MARRIED**\n",
    "- MARRIAGE_2.0 → **MARRIAGE_SINGLE**\n",
    "- MARRIAGE_3.0 → **MARRIAGE_OTHER**\n",
    "\n",
    "Each dummy variable takes the value:\n",
    "- 1 if the observation belongs to that category\n",
    "- 0 otherwise\n",
    "\n",
    "The original **MARRIAGE** column was automatically removed by the get_dummies() function, and the data is now ready for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q5**. In the column 'EDUCATION', convert the values {0, 5, 6} to the value 4. \n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "# Replace 0, 5, and 6 in EDUCATION with 4 (Others)\n",
    "df['EDUCATION'] = df['EDUCATION'].replace([0, 5, 6], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 3** Preparing X and y arrays (Total Marks: 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. \n",
    "\n",
    "- Create a numpy array `y` from the first 7,000 observations of `default payment next month` column from `df` (2.5 marks)   \n",
    "- Create a numpy array `X`  from the first 7,000 observations of all the remaining variables in `df` (2.5 marks)   \n",
    "\n",
    "(Total: 5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "\n",
    "# Create X and y arrays from the first 7,000 observations\n",
    "X = df.drop(columns=['default payment next month']).iloc[:7000]\n",
    "y = df['default payment next month'].iloc[:7000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. \n",
    "\n",
    "- Use an appropriate `sklearn` library we used in class to create `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 70% train and 30% test datasets (2.5 marks) \n",
    "    - Set random_state to 31 and stratify the subsamples so that train and test datasets have roughly equal proportions of the target's class labels \n",
    "- Standardise the data to mean zero and variance one using an approapriate `sklearn` library (2.5 marks)   \n",
    "\n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=31,stratify=y)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardise the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Fit on train, transform train\n",
    "X_test = scaler.transform(X_test)        # Transform test with same scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 4**. Training Models and Interpretation (Total Marks: 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. \n",
    "\n",
    "- Train one linear classifier we studied in class using standardised data (6 marks)\n",
    "- Compute and print training and test dataset accuracies (4 marks)\n",
    "\n",
    "(Total: 10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 82.06%\n",
      "Test Accuracy: 80.52%\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=31)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction \n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print Accuracy\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2.**\n",
    "\n",
    "- Train one nonlinear classifier we studied in class on the same dataset (6 marks)\n",
    "- Compute and print training and test dataset accuracies (4 marks)\n",
    "\n",
    "\n",
    "(Total: 10 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.98%\n",
      "Test Accuracy: 80.71%\n"
     ]
    }
   ],
   "source": [
    "# ---- provide answer here -----\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=31)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracies\n",
    "rf_train_accuracy = rf_model.score(X_train, y_train)\n",
    "rf_test_accuracy = rf_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {rf_train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {rf_test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3**. \n",
    "\n",
    "- Comment on the accuracy results obtained from the two classifiers (6 marks)\n",
    "- Based on our investigation into credit card default predictions in this assignment, which model would you recommend? (4 marks)\n",
    "\n",
    "\n",
    "(Total: 10 marks)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "**Comment on the Accuracy Results**\n",
    "\n",
    "In this task, I trained two types of classifiers based on the approaches studied in class:\n",
    "\n",
    "- **Linear classifier**: Logistic Regression  \n",
    "  - Training Accuracy: 82.06%  \n",
    "  - Testing Accuracy: 80.52%\n",
    "\n",
    "- **Nonlinear classifier**: Random Forest  \n",
    "  - Training Accuracy: 99.98%  \n",
    "  - Testing Accuracy: 80.71%\n",
    "\n",
    "The results show that the **nonlinear classifier (Random Forest)** achieved slightly higher test accuracy than the linear classifier. However, the large gap between training and testing accuracy indicates **overfitting**, meaning the model fits the training data too closely and may not generalize well.\n",
    "\n",
    "In contrast, the **linear classifier (Logistic Regression)** shows more balanced results between training and testing accuracy, suggesting **better generalization** to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "**Model Recommendation**\n",
    "\n",
    "Although Logistic Regression is more stable and interpretable, I recommend using the **nonlinear classifier (Random Forest)** because:\n",
    "\n",
    "- It achieves the **highest test accuracy**  \n",
    "- It can model **complex relationships** between features  \n",
    "- It is suitable for this dataset, which includes both numerical and categorical features\n",
    "\n",
    "**Conclusion:**  \n",
    "The nonlinear classifier is more suitable for this task due to its higher predictive performance. However, attention should be given to the risk of overfitting when applying this model in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking Criteria\n",
    "\n",
    "To achieve a perfect score, your solutions must adhere to the criteria outlined below:\n",
    "\n",
    "- Ensure that all numerical answers are accurate.\n",
    "- Utilize the exact Python functions and libraries specified within the assignment instructions.\n",
    "- For any written responses, provide accurate information, articulated in clear, complete sentences.\n",
    "- Do not add extra cells beyond what is provided in the notebook.\n",
    "- Do not print output with your code unless explicitly instructed to do so.\n",
    "- Maintain a clean and organised notebook layout that is easy to follow.\n",
    "- Marks will be deducted for not following the above instructions.\n",
    "    \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
